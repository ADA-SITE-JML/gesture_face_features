{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM0z0F+Pdal2GEXZcS0JvZX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3rR1dYK3sGrk","executionInfo":{"status":"ok","timestamp":1734371380308,"user_tz":-240,"elapsed":23288,"user":{"displayName":"Ismayil Shahaliyev","userId":"06401584431432004248"}},"outputId":"8f0dc9b4-ed4b-4655-933c-514586638a78"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import sys\n","import os\n","\n","# modify the project path below accordingly\n","PATH = \"/content/drive/MyDrive/HGR/gesture_face_features\"\n","\n","sys.path.append(os.path.join(PATH, \"code\"))\n","\n","%load_ext autoreload\n","%autoreload 2"],"metadata":{"id":"6VfiqlJIsMwa","executionInfo":{"status":"ok","timestamp":1734371380308,"user_tz":-240,"elapsed":5,"user":{"displayName":"Ismayil Shahaliyev","userId":"06401584431432004248"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["data_path = os.path.join(PATH, 'samples', 'sign')\n","feat_path = os.path.join(PATH, 'feats')"],"metadata":{"id":"tzntyQVssNxb","executionInfo":{"status":"ok","timestamp":1734371380308,"user_tz":-240,"elapsed":4,"user":{"displayName":"Ismayil Shahaliyev","userId":"06401584431432004248"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","import torch\n","from torch.utils.data import DataLoader\n","\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torchvision.models.feature_extraction import create_feature_extractor"],"metadata":{"id":"xLKD6M-gt0HD","executionInfo":{"status":"ok","timestamp":1734371389979,"user_tz":-240,"elapsed":9675,"user":{"displayName":"Ismayil Shahaliyev","userId":"06401584431432004248"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class SignImageFolder(ImageFolder):\n","  def __init__(self, root, transform=transforms.Compose([\n","        transforms.Resize((600, 600), transforms.InterpolationMode.BICUBIC),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                              std=[0.229, 0.224, 0.225]),\n","      ])):\n","    super().__init__(root, transform=transform)\n","    self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n","    self.dims = []\n","\n","  def __getitem__(self, index):\n","    image, label = super().__getitem__(index)\n","    image_path = self.samples[index][0]\n","    filename = os.path.basename(image_path)\n","    image_id = int(filename.split('_')[1].replace('.JPG', ''))\n","\n","    return image, label, image_id\n","\n","  def denormalize(self, tensor):\n","    inv_normalize = transforms.Normalize(\n","        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n","        std=[1/0.229, 1/0.224, 1/0.225]\n","    )\n","    return inv_normalize(tensor)\n","\n","  def plot(self, id):\n","      tensor, class_name, img_id = self.__getitem__(id)\n","      tensor = self.denormalize(tensor).clamp(0,1)\n","      image = tensor.permute(1,2,0).numpy()\n","\n","      plt.imshow(image)\n","      plt.title(f\"{self.idx_to_class[class_name]}:{img_id}\")\n","      plt.show()"],"metadata":{"id":"OcM7dQhKvw5G","executionInfo":{"status":"ok","timestamp":1734371389980,"user_tz":-240,"elapsed":3,"user":{"displayName":"Ismayil Shahaliyev","userId":"06401584431432004248"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class ModelLoader:\n","  available_models = ['vgg19','resnet50','inception_v3',\n","                      'efficientnet_b0','efficientnet_b1','efficientnet_b6']\n","  def __init__(self, model_name):\n","    assert model_name in self.available_models\n","    self.model_name = model_name\n","    self.model, self.layers = self.load()\n","    self.input_dim = self.input_dims[model_name]\n","\n","  def load(self):\n","      '''\n","        https://jacobgil.github.io/pytorch-gradcam-book/introduction.html\n","\n","        Resnet18 and 50: model.layer4[-1]\n","        VGG and densenet161: model.features[-1]\n","        ViT: model.blocks[-1].norm1\n","        SwinT: model.layers[-1].blocks[-1].norm1\n","\n","        Are suggested for last convolutional layers.\n","      '''\n","      if self.model_name == 'vgg19':\n","          from torchvision.models import vgg19, VGG19_Weights\n","          model = vgg19(weights=VGG19_Weights.DEFAULT)\n","          layers = (model.features[34], model.avgpool)\n","      elif self.model_name == 'resnet50':\n","          from torchvision.models import resnet50, ResNet50_Weights\n","          model = resnet50(weights=ResNet50_Weights.DEFAULT)\n","          layers = (model.layer4[2].conv3, model.avgpool)\n","      elif self.model_name == 'inception_v3':\n","          from torchvision.models import inception_v3, Inception_V3_Weights\n","          model = inception_v3(weights=Inception_V3_Weights.DEFAULT)\n","          layers = (model.Mixed_7c.branch_pool.conv, model.avgpool)\n","      elif self.model_name == 'efficientnet_b0':\n","          from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n","          model = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n","          layers = (model.features[8][0], model.avgpool)\n","      elif self.model_name == 'efficientnet_b1':\n","          from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n","          model = efficientnet_b1(weights=EfficientNet_B1_Weights.DEFAULT)\n","          layers = (model.features[8][0], model.avgpool)\n","      elif self.model_name == 'efficientnet_b6':\n","          from torchvision.models import efficientnet_b6, EfficientNet_B6_Weights\n","          model = efficientnet_b6(weights=EfficientNet_B6_Weights.DEFAULT)\n","          layers = (model.features[8][0], model.avgpool)\n","      model.eval()\n","      return model, layers\n","\n","  # https://discuss.pytorch.org/t/how-to-get-input-shape-of-model/85877/4\n","  input_dims = {\n","    'vgg19': (224, 224),\n","    'resnet50': (224, 224),\n","    'inception_v3': (299, 299),\n","    'efficientnet_b0': (224, 224),\n","    'efficientnet_b1': (240, 240),\n","    'efficientnet_b6': (528, 528),\n","  }"],"metadata":{"id":"Y4H9vSC-NaZ6","executionInfo":{"status":"ok","timestamp":1734372255644,"user_tz":-240,"elapsed":444,"user":{"displayName":"Ismayil Shahaliyev","userId":"06401584431432004248"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["dataset = SignImageFolder(data_path)\n","dataloader = DataLoader(dataset, batch_size=32, num_workers=2)"],"metadata":{"id":"d42-lv7OFaCB","executionInfo":{"status":"ok","timestamp":1734371391997,"user_tz":-240,"elapsed":2020,"user":{"displayName":"Ismayil Shahaliyev","userId":"06401584431432004248"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import torch\n","import os\n","\n","class FeatureExtractor:\n","    def __init__(self, modelloader, dataloader, feat_path, layer_type='gap'):\n","        assert layer_type in ['conv', 'gap']\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.m = modelloader\n","        self.dataloader = dataloader\n","        self.feats = {}\n","        self.feat_path = feat_path\n","        self.layer_type = layer_type\n","        self.layer = self.m.layers[0] if self.layer_type == 'conv' else self.m.layers[1]\n","        self.path = os.path.join(feat_path, f\"{self.m.model_name}_{self.layer_type}_layer_feats.pt\")\n","\n","    def _hook_fn(self, module, input, output):\n","        for idx, img_id in enumerate(self.img_ids):\n","            self.feats[img_id.item()] = output[idx]\n","\n","    def extract_features(self, save):\n","        print(f'Extracting features for {self.m.model_name} {self.layer} layer.')\n","        self.m.model.to(self.device)\n","\n","        self.hook = self.layer.register_forward_hook(self._hook_fn)\n","        for images, labels, img_ids in self.dataloader:\n","            self.img_ids = img_ids\n","            images = images.to(self.device)\n","            with torch.no_grad():\n","                output = self.m.model(images)\n","            print(f\"Extracted ids for this batch: {img_ids}\")\n","        self.hook.remove()\n","\n","        if save:\n","          self.save_features()\n","\n","    def save_features(self):\n","        torch.save(self.feats, self.path)\n","        print(f\"Features saved to {self.path}\")\n","\n","    def load_features(self):\n","        if os.path.exists(self.path):\n","            loaded_feats = torch.load(self.path, weights_only=True)\n","            self.feats = loaded_feats\n","            print(f\"Features loaded from {self.path}\")\n","            return True\n","        else:\n","            print(f\"No features found at {self.path}. For computing features call extract_features().\")\n","            return False\n","\n","def get_all_feats(layer_type, save=True):\n","  feats = {}\n","  for model_name in ModelLoader.available_models:\n","    m = ModelLoader(model_name)\n","    fe = FeatureExtractor(m, dataloader, feat_path, layer_type)\n","    if not fe.load_features():\n","      fe.extract_features(save=save)\n","    feats[model_name] = fe.feats\n","  return feats"],"metadata":{"id":"Po_OXyWzblP1","executionInfo":{"status":"ok","timestamp":1734372826203,"user_tz":-240,"elapsed":8,"user":{"displayName":"Ismayil Shahaliyev","userId":"06401584431432004248"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hZp9Y5QZjy1q","executionInfo":{"status":"ok","timestamp":1734373076148,"user_tz":-240,"elapsed":5015,"user":{"displayName":"Ismayil Shahaliyev","userId":"06401584431432004248"}},"outputId":"f1962022-237e-40d5-d75f-843827d23f79"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/vgg19_gap_layer_feats.pt\n","Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/resnet50_gap_layer_feats.pt\n","Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/inception_v3_gap_layer_feats.pt\n","Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/efficientnet_b0_gap_layer_feats.pt\n","Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/efficientnet_b1_gap_layer_feats.pt\n","Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/efficientnet_b6_gap_layer_feats.pt\n"]}]},{"cell_type":"code","source":["class FeatureExtractor:\n","    def __init__(self, modelloader, dataloader, feat_path):\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.m = modelloader\n","        self.dataloader = dataloader\n","        self.feat_path = feat_path\n","        self.conv_layer_feats = {}\n","        self.gap_layer_feats = {}\n","        self.paths = {\n","            \"conv\": os.path.join(self.feat_path, f\"{self.m.model_name}_conv_layer_feats.pt\"),\n","            \"gap\": os.path.join(self.feat_path, f\"{self.m.model_name}_gap_layer_feats.pt\")\n","        }\n","\n","    def _hook_fn_conv(self, module, input, output):\n","        for idx, img_id in enumerate(self.img_ids):\n","            self.conv_layer_feats[img_id.item()] = output[idx]\n","\n","    def _hook_fn_gap(self, module, input, output):\n","        for idx, img_id in enumerate(self.img_ids):\n","            self.gap_layer_feats[img_id.item()] = output[idx]\n","\n","    def extract_features(self, save):\n","        print(f'Extracting features for {self.m.model_name} layers.')\n","        self.m.model.to(self.device)\n","\n","        self.hook1 = self.m.layers[0].register_forward_hook(self._hook_fn_conv)\n","        self.hook2 = self.m.layers[1].register_forward_hook(self._hook_fn_gap)\n","\n","        for images, labels, img_ids in self.dataloader:\n","            self.img_ids = img_ids\n","            images = images.to(self.device)\n","            with torch.no_grad():\n","                output = self.m.model(images)\n","            print(f\"Extracted ids for this batch: {img_ids}\")\n","\n","        self.hook1.remove()\n","        self.hook2.remove()\n","\n","        if save:\n","            self.save_features()\n","\n","    def save_features(self):\n","        torch.save(self.conv_layer_feats, self.paths['conv'])\n","        print(f\"Conv layer features saved to {self.paths['conv']}\")\n","        torch.save(self.gap_layer_feats, self.paths['gap'])\n","        print(f\"Gap layer features saved to {self.paths['gap']}\")\n","\n","    def load_features(self, layer_type):\n","        assert layer_type in ['conv', 'gap']\n","        if os.path.exists(self.paths[layer_type]):\n","            loaded_feats = torch.load(self.paths[layer_type], weights_only=True)\n","            if layer_type == 'conv':\n","                self.conv_layer_feats = loaded_feats\n","            elif layer_type == 'gap':\n","                self.gap_layer_feats = loaded_feats\n","            print(f\"Features loaded from {self.paths[layer_type]}\")\n","            return True\n","        else:\n","            print(f\"No features found at {self.paths[layer_type]}. For computing features call extract_features().\")\n","            return False\n","\n","def get_all_feats(save=True):\n","    feats_conv = {}\n","    feats_gap = {}\n","    for model_name in ModelLoader.available_models:\n","        m = ModelLoader(model_name)\n","        fe = FeatureExtractor(m, dataloader, feat_path)\n","\n","        if not fe.load_features('conv'):\n","            fe.extract_features(save=save)\n","        feats_conv[model_name] = fe.conv_layer_feats\n","        if not fe.load_features('gap'):\n","            fe.extract_features(save=save)\n","        feats_gap[model_name] = fe.gap_layer_feats\n","\n","    return feats_conv, feats_gap"],"metadata":{"id":"7AGR-Z_HsP9k","executionInfo":{"status":"ok","timestamp":1734374770295,"user_tz":-240,"elapsed":1015,"user":{"displayName":"Ismayil Shahaliyev","userId":"06401584431432004248"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["feats_conv, feats_gap = get_all_feats()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"so3yz5bcywkb","executionInfo":{"status":"ok","timestamp":1734375161778,"user_tz":-240,"elapsed":13810,"user":{"displayName":"Ismayil Shahaliyev","userId":"06401584431432004248"}},"outputId":"1ef0dea9-96e3-4792-ba50-4e1fdde1b805"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/vgg19_conv_layer_feats.pt\n","Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/vgg19_gap_layer_feats.pt\n","Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/resnet50_conv_layer_feats.pt\n","Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/resnet50_gap_layer_feats.pt\n","Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/inception_v3_conv_layer_feats.pt\n","Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/inception_v3_gap_layer_feats.pt\n","Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/efficientnet_b0_conv_layer_feats.pt\n","Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/efficientnet_b0_gap_layer_feats.pt\n","Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/efficientnet_b1_conv_layer_feats.pt\n","Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/efficientnet_b1_gap_layer_feats.pt\n","Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/efficientnet_b6_conv_layer_feats.pt\n","Features loaded from /content/drive/MyDrive/HGR/gesture_face_features/feats/efficientnet_b6_gap_layer_feats.pt\n"]}]},{"cell_type":"code","source":["feats_gap['vgg19'][1539]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uIAJHRrS0MzN","executionInfo":{"status":"ok","timestamp":1734375296657,"user_tz":-240,"elapsed":925,"user":{"displayName":"Ismayil Shahaliyev","userId":"06401584431432004248"}},"outputId":"2bca344c-87fd-4dd2-a071-d880ec234cee"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         ...,\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00]],\n","\n","        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 7.9675e-02,  ..., 2.3662e-01,\n","          1.2576e-01, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 2.5457e-01,  ..., 5.8608e-02,\n","          4.3956e-02, 0.0000e+00],\n","         ...,\n","         [0.0000e+00, 6.8307e-01, 4.0275e-01,  ..., 1.3315e-01,\n","          0.0000e+00, 0.0000e+00],\n","         [2.0557e-01, 7.1693e-02, 3.5058e-02,  ..., 1.1694e-01,\n","          1.4666e-01, 0.0000e+00],\n","         [0.0000e+00, 5.4756e-02, 4.6744e-02,  ..., 1.0356e-01,\n","          8.9249e-01, 0.0000e+00]],\n","\n","        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 1.2770e-01, 1.3173e+00,  ..., 2.4591e+00,\n","          3.7251e-01, 0.0000e+00],\n","         ...,\n","         [0.0000e+00, 0.0000e+00, 2.2599e-03,  ..., 4.5178e-01,\n","          5.3062e-02, 0.0000e+00],\n","         [2.3564e-01, 7.1503e-01, 0.0000e+00,  ..., 7.3379e-02,\n","          1.9329e-02, 0.0000e+00],\n","         [1.2497e-01, 3.8342e-01, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00]],\n","\n","        ...,\n","\n","        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         ...,\n","         [0.0000e+00, 9.9325e-01, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 2.7078e-01, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00]],\n","\n","        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         ...,\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.4437e-01,\n","          5.1478e-02, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0189e-01,\n","          1.4192e-02, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00]],\n","\n","        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         ...,\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00],\n","         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n","          0.0000e+00, 0.0000e+00]]], device='cuda:0')"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["participants = {\n","    0: list(range(2914, 2951)),\n","    1: list(range(2871, 2904)),\n","    2: list(range(2323, 2356)),\n","    3: list(range(2285, 2314)),\n","    4: list(range(1646, 1675)),\n","    5: list(range(1503, 1535)) + list(range(1537, 1544)),\n","}\n","p0_feats = {img_id: data for img_id, data in feats.items() if img_id in participants[0]}"],"metadata":{"id":"PyAy54gtburu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(p0_feats)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hE1JfgxKgU44","executionInfo":{"status":"ok","timestamp":1734303301597,"user_tz":-240,"elapsed":281,"user":{"displayName":"Ismayil Shahaliyev","userId":"06401584431432004248"}},"outputId":"a7950b09-80ff-4813-a942-0bee84dfa324"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["37"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":[],"metadata":{"id":"Y7-w2V7SiD0w"},"execution_count":null,"outputs":[]}]}